{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Processing SS 18 - Assignment - 09\n",
    "\n",
    "### Deadline is 20.6.2016 at 8:00 o'clock\n",
    "\n",
    "Please solve the assignments together with a partner.\n",
    "I will run every notebook. Make sure the code runs through. Select `Kernel` -> `Restart & Run All` to test it.\n",
    "Please strip the output from the cells, either select `Cell` -> `All Output` -> `Clear` or use the `nb_strip_output.py` script / git hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the plots inside the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import scipy.io.wavfile\n",
    "from skimage.data import astronaut\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from __future__ import division\n",
    "import random\n",
    "try:\n",
    "    from StringIO import StringIO as BytesIO\n",
    "except ImportError:\n",
    "    from io import BytesIO\n",
    "    \n",
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2\n",
    "    \n",
    "    \n",
    "from numpy.fft import fft2 as numpy_fft2, ifft2 as numpy_ifft2\n",
    "from scipy.fftpack import dctn, idctn\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import IPython\n",
    "import zipfile\n",
    "pylab.rcParams['figure.figsize'] = (12, 12)   # This makes the plot bigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - JPEG - 10 Points\n",
    "\n",
    "The Wikipedia page about [JPEG-Komprimierung](https://de.wikipedia.org/wiki/JPEG#Die_JPEG-Komprimierung) gives\n",
    "a good overview. The Wikipedia pages also assumes that all images are between 0 and 255. That is why we do not convert it to 0-1 as an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = astronaut() # The \n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(img); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.min(img), np.max(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Blocks:\n",
    "    \"\"\"Transforms an image to blocks. A (512, 512) image will become an (64, 64, 8, 8) numpy array\"\"\"\n",
    "    def __init__(self, block_size=8):\n",
    "        self.block_size = block_size\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        b = self.block_size\n",
    "        heigth, width = img.shape\n",
    "        assert img.shape[0] % b == 0\n",
    "        assert img.shape[1] % b == 0\n",
    "        blocks = np.zeros((heigth // b, width // b, b, b), dtype=img.dtype)\n",
    "        for i in range(0, heigth // b):\n",
    "            for j in range(0, width // b):\n",
    "                blocks[i, j] = img[i*b:(i+1)*b, j*b:(j+1)*b]\n",
    "        return blocks\n",
    "\n",
    "    def invert(self, blocks):\n",
    "        bh, bw = blocks.shape[:2]\n",
    "        b = self.block_size\n",
    "        heigth, width = (bh*self.block_size, bw*self.block_size)\n",
    "\n",
    "        img = np.zeros((heigth, width), dtype=blocks.dtype)\n",
    "        for i in range(0, bh):\n",
    "            for j in range(0, bw):\n",
    "                img[i*b:(i+1)*b, j*b:(j+1)*b] = blocks[i, j]\n",
    "        return img\n",
    "    \n",
    "\n",
    "img_blocks = Blocks(block_size=8)(img[:, :, 0])\n",
    "print(img_blocks.shape)\n",
    "assert img_blocks.shape[2:] == (8, 8)\n",
    "img_inv = Blocks(block_size=8).invert(img_blocks)\n",
    "assert (img[:, :, 0] == img_inv).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ChromaSubsampling:\n",
    "    \"\"\"See https://en.wikipedia.org/wiki/YCbCr.\"\"\"\n",
    "    ycbcr = np.array([\n",
    "        [0.299,      0.587,     0.114],\n",
    "        [-0.168736, -0.331264,  0.5],\n",
    "        [0.5,       -0.418688, -0.081312],\n",
    "    ])\n",
    "    \n",
    "    renorm = np.array([[0.,128.,128.]]).T\n",
    "    \n",
    "    step = 2\n",
    "    \n",
    "    def __call__(self, rgb_img):\n",
    "        \"\"\"Transforms the rgb image to YCbCr. The cb and cr channels have half the resolution of the Y-channel.\n",
    "           You can simply use the mean of four neighbours.\n",
    "        \"\"\"\n",
    "        shape = rgb_img.shape\n",
    "        ycbcr_img = np.dot(self.ycbcr, rgb_img.reshape(-1, 3).T) + self.renorm\n",
    "        ycbcr_img = ycbcr_img.T.reshape(shape)\n",
    "        # subsample the cb and cr channel, so that they have half the resolution of the Y-channel.\n",
    "        # A simple thing might be to use the mean of 4 neighbours.\n",
    "        \n",
    "        # assumption: cb and cr channel have \"half the resolution of the Y-channel\" means, that\n",
    "        # both together have half the size of the Y-channel. So each has a 4th of the resolution.\n",
    "        # -> then a mean of 4 neighbors is OK.\n",
    "        # So 4 neighbors -> apply mean of 2by2 patches of channel 1(Cb) and 2(Cr) (4:2:0)\n",
    "        stp = self.step\n",
    "        for i in range(0, ycbcr_img.shape[0], stp):\n",
    "            for j in range(0, ycbcr_img.shape[1], stp):\n",
    "                ycbcr_img[i:i+stp, j:j+stp, 1] = np.mean(ycbcr_img[i:i+stp, j:j+stp, 1])\n",
    "                ycbcr_img[i:i+stp, j:j+stp, 2] = np.mean(ycbcr_img[i:i+stp, j:j+stp, 2])\n",
    "        return ycbcr_img[:, :, 0], ycbcr_img[:, :, 1][::stp,::stp], ycbcr_img[:, :, 2][::stp,::stp]\n",
    "    \n",
    "    def invert(self, inputs):\n",
    "        y, cb, cr = inputs\n",
    "        stp = self.step\n",
    "        assert cb.shape == cr.shape\n",
    "        chr_shape = cb.shape\n",
    "        new_shape = (stp*chr_shape[0], stp*chr_shape[1])\n",
    "        assert new_shape == y.shape\n",
    "        cb_os = np.zeros(new_shape)\n",
    "        cr_os = np.zeros(new_shape)\n",
    "        for i in range(chr_shape[0]):\n",
    "            for j in range(chr_shape[1]):\n",
    "                cb_os[stp*i:stp*i+stp, stp*j:stp*j+stp] = cb[i,j]\n",
    "                cr_os[stp*i:stp*i+stp, stp*j:stp*j+stp] = cr[i,j] \n",
    "        ycbcr_img = np.stack([y, cb_os, cr_os], axis=-1)\n",
    "        shape = ycbcr_img.shape        \n",
    "        rgb_img = np.dot(np.linalg.inv(self.ycbcr), (ycbcr_img.reshape(-1, 3).T - self.renorm))\n",
    "        rgb_img = rgb_img.T.reshape(shape)\n",
    "        np.clip(rgb_img, 0, 255, out=rgb_img)\n",
    "        return rgb_img\n",
    "    \n",
    "class DCTofBlocks:\n",
    "    def __call__(self, blocks):\n",
    "        \"\"\"Returns the DCT of the blocks. The position (i, j) is a 2-dim numpy array with the dct coefficents.\"\"\"\n",
    "        # you can use any function from np.fft or scipy.fftpack\n",
    "        dct_blocks = np.zeros_like(blocks)\n",
    "        for i in range(blocks.shape[0]):\n",
    "            for j in range(blocks.shape[1]):\n",
    "                dct_blocks[i, j] = dctn(blocks[i, j], norm='ortho')\n",
    "        return dct_blocks\n",
    "    \n",
    "    def invert(self, blocks):\n",
    "        \"\"\"Computes the inverse DCT.\"\"\"\n",
    "        # you can use any function from np.fft or scipy.fftpack\n",
    "        ycbcr_blocks = np.zeros_like(blocks)\n",
    "        for i in range(blocks.shape[0]):\n",
    "            for j in range(blocks.shape[1]):\n",
    "                ycbcr_blocks[i, j] = idctn(blocks[i, j], norm='ortho')\n",
    "        return ycbcr_blocks\n",
    "    \n",
    "class Quantizise:\n",
    "    def __init__(self, threshold=1):\n",
    "        self.q_matrix = np.array([[16,11,10,16,24,40,51,61],\n",
    "                                  [12,12,14,19,26,48,60,55],\n",
    "                                  [14,13,16,24,40,57,69,56],\n",
    "                                  [14,17,22,29,51,87,80,62],\n",
    "                                  [18,22,37,56,68,109,103,77],\n",
    "                                  [24,35,55,64,81,104,113,92],\n",
    "                                  [49,64,78,87,103,121,120,101],\n",
    "                                  [72,92,95,98,112,100,103,99]])\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, blocks):\n",
    "        \"\"\"Divides the blocks by the `q_matrix` elementwise. Coefficents under the `threshold` will be set to zero.\"\"\"\n",
    "        quant_blocks = np.zeros_like(blocks)\n",
    "        for i in range(blocks.shape[0]):\n",
    "            for j in range(blocks.shape[1]):\n",
    "                quant_blocks[i, j] = blocks[i, j] / self.q_matrix\n",
    "        quant_blocks[np.abs(quant_blocks) < self.threshold] = 0\n",
    "        quant_blocks = np.rint(quant_blocks)\n",
    "        print(\"mean nb of freq. left: %.1f\"\n",
    "              % (np.sum(np.abs(quant_blocks) > 0) / (blocks.shape[0]*blocks.shape[1])))\n",
    "        return quant_blocks\n",
    "    \n",
    "    def invert(self, blocks):\n",
    "        \"\"\" For inverting multiply your elements piecewise with the Q-Matrix\"\"\"\n",
    "        requant_blocks = np.zeros_like(blocks)\n",
    "        for i in range(blocks.shape[0]):\n",
    "            for j in range(blocks.shape[1]):\n",
    "                requant_blocks[i, j] = blocks[i, j] * self.q_matrix\n",
    "        return requant_blocks\n",
    "\n",
    "class PickNthHighest():\n",
    "    def __init__(self, n=10):\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, blocks):\n",
    "        \"\"\"Pick the nth-highest frequencies\"\"\"\n",
    "        highest_blocks = np.copy(blocks)\n",
    "        for i in range(blocks.shape[0]):\n",
    "            for j in range(blocks.shape[1]):\n",
    "                limit = (np.sort(np.ravel(np.abs(blocks[i, j]))))[-self.n]\n",
    "                highest_blocks[i, j][np.abs(highest_blocks[i, j]) < limit] = 0\n",
    "        highest_blocks = np.rint(highest_blocks)\n",
    "        print(\"mean nb of freq. left: %.1f\"\n",
    "              % (np.sum(np.abs(highest_blocks) > 0) / (blocks.shape[0]*blocks.shape[1])))\n",
    "        return highest_blocks\n",
    "    \n",
    "    def invert(self, blocks):\n",
    "        \"\"\"There is no way to invert this operation. Just return the inputs.\"\"\"\n",
    "        return blocks\n",
    "\n",
    "plt.imshow(ChromaSubsampling()(astronaut())[1], cmap='gray')\n",
    "plt.title(\"subsampled Cb-Channel\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ZigZack:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        \"\"\"Adapted from here: https://rosettacode.org/wiki/Zig-zag_matrix#Python\"\"\"\n",
    "        def key(pair):\n",
    "            x, y = pair\n",
    "            return (x+y, -y if (x+y) % 2 else y)\n",
    "\n",
    "        n = 8\n",
    "        indexorder = sorted(((x,y) for x in range(n) for y in range(n)), key=key)\n",
    "        self.xs = np.zeros((self.n**2,), dtype=np.int)\n",
    "        self.ys = np.zeros((self.n**2,), dtype=np.int)\n",
    "        self.back = np.zeros((n, n), dtype=np.int)\n",
    "        for i, (x, y) in enumerate(indexorder):\n",
    "            self.xs[i] = x\n",
    "            self.ys[i] = y\n",
    "            self.back[x, y] = i\n",
    "            \n",
    "    def __call__(self, blocks):\n",
    "        bh, bw, h, w = blocks.shape\n",
    "        zigzack_blocks = np.zeros((bh, bw, h*w), dtype=blocks.dtype)\n",
    "        for i, block_row in enumerate(blocks):\n",
    "            for j, block in enumerate(block_row):\n",
    "                zigzack_blocks[i, j] = block[self.xs, self.ys] \n",
    "        return zigzack_blocks\n",
    "    \n",
    "    def invert(self, zigzack_blocks):\n",
    "        bh, bw, hw = zigzack_blocks.shape\n",
    "        h = int(np.sqrt(hw))\n",
    "        blocks = np.zeros((bh, bw, h, h), dtype=zigzack_blocks.dtype)\n",
    "        for i, zigzack_row in enumerate(zigzack_blocks):\n",
    "            for j, zigzack in enumerate(zigzack_row):\n",
    "                blocks[i, j] = zigzack[self.back] \n",
    "        return blocks\n",
    "\n",
    "zigzag = ZigZack(8)\n",
    "range_mat = np.arange(64).reshape(1, 1, 8, 8)\n",
    "zigzack_mat = zigzag(range_mat)\n",
    "assert (zigzag.invert(zigzack_mat) == range_mat).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Compress:\n",
    "    def __init__(self, dtype=np.int8):\n",
    "        self.dtype = dtype\n",
    "        self.max_value = (np.iinfo(dtype).max  / 1.1)\n",
    "        \n",
    "    def __call__(self, arr):\n",
    "        # print(\"dtype: {}\".format(arr.dtype))\n",
    "        # print(\"max: {}, min: {}\".format(arr.max(), arr.min()))\n",
    "        scale = max(abs(arr.max()), abs(arr.min()))\n",
    "        arr = arr / scale \n",
    "        arr = arr * self.max_value\n",
    "        arr = np.rint(arr)\n",
    "        arr = arr.astype(self.dtype)\n",
    "        bytearr = arr.data.tobytes()\n",
    "        \n",
    "        return zipfile.bz2.compress(bytearr), arr.shape, arr.dtype, scale\n",
    "    \n",
    "    def invert(self, inputs):\n",
    "        bytearr, shape, dtype, scale = inputs\n",
    "        decom_bytes = zipfile.bz2.decompress(bytearr)\n",
    "        arr = np.frombuffer(decom_bytes, dtype=self.dtype)\n",
    "        arr = arr.astype(dtype)\n",
    "        arr = arr / self.max_value * scale\n",
    "        return arr.reshape(shape)\n",
    "\n",
    "range_mat = np.arange(64, dtype=np.float64).reshape(1, 1, 8, 8)\n",
    "compress = Compress()\n",
    "compressed = compress(range_mat)\n",
    "# print((compress.invert(compressed)))\n",
    "assert np.allclose(compress.invert(compressed), range_mat, 0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Jpeg:\n",
    "    def __init__(self, stages):\n",
    "        self.stages = stages\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        y, cb, cr = ChromaSubsampling()(img)\n",
    "        outputs = []\n",
    "        for input in [y, cb, cr]:\n",
    "            output = input\n",
    "            for stage in self.stages:\n",
    "                input = output\n",
    "                try:\n",
    "                    output = stage(input)\n",
    "                except:\n",
    "                    print(\"Error in Stage: {}\".format(type(stage).__name__))\n",
    "                    raise\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "    \n",
    "    def invert(self, inputs):\n",
    "        outputs = []\n",
    "        for output in inputs:\n",
    "            for stage in self.stages[::-1]:\n",
    "                input = output\n",
    "                try:\n",
    "                    output = stage.invert(input)\n",
    "                except:\n",
    "                    print(\"Error in Stage: {}\".format(type(stage).__name__))\n",
    "                    raise\n",
    "            outputs.append(output)\n",
    "        y, cb, cr = outputs\n",
    "        return ChromaSubsampling().invert([y, cb, cr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_size_jpeg(jpeg_output):\n",
    "    \"\"\"Summs the number of bytes over the different compression channels: y, cb, cr\"\"\"\n",
    "    nb_bytes = sum([len(x[0]) for x in jpeg_output])\n",
    "    return \"{:.1f}KB\".format(nb_bytes / 1000)\n",
    "\n",
    "def total_size_jpeg_nb(jpeg_output):\n",
    "    \"\"\"Summs the number of bytes over the different compression channels: y, cb, cr\"\"\"\n",
    "    nb_bytes = sum([len(x[0]) for x in jpeg_output])\n",
    "    return nb_bytes / 1000\n",
    "\n",
    "def total_size_numpy(arr):\n",
    "    return \"{:.1f}KB\".format(len(arr.data.tobytes()) / 1000)\n",
    "\n",
    "def naive_compression_size(arr):\n",
    "    bytearr = arr.data.tobytes()\n",
    "    nb_bytes = len(zipfile.bz2.compress(bytearr))\n",
    "    return \"{:.1f}KB\".format(nb_bytes / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build the jpeg pipeline\n",
    "# for testing you can use only the first ones.\n",
    "# maybe you have to adjust the Quantizise.threshold settings.\n",
    "jpeg = Jpeg([Blocks(8), DCTofBlocks(), Quantizise(threshold=1), ZigZack(8), Compress(np.int8)])\n",
    "\n",
    "img_jpeg = jpeg(img)\n",
    "img_reconstruct = jpeg.invert(img_jpeg)\n",
    "assert img_reconstruct.shape == img.shape\n",
    "# once you implemted ChormaSubsampling.invert this should have colors :)\n",
    "plt.imshow(img_reconstruct / 255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chromasubsampling_compression = Jpeg([Blocks(8), Compress()])\n",
    "print(\"No compression: \" + total_size_numpy(img))\n",
    "print(\"Direct compression of the image: \" + naive_compression_size(img))\n",
    "print(\"Cromasubsampling and compression: \" + total_size_jpeg(chromasubsampling_compression(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare the size of the images if the zigzack encoding is removed.\n",
    "# Does the size change if the Quantizise.threshold increases?\n",
    "def compareZigZag(threshold):\n",
    "    jpegLIN = Jpeg([Blocks(8), DCTofBlocks(), Quantizise(threshold=threshold), Compress(np.uint8)])\n",
    "    jpegZZ = Jpeg([Blocks(8), DCTofBlocks(), Quantizise(threshold=threshold), ZigZack(8), Compress(np.int8)])\n",
    "    img_jpgZZ = jpegZZ(img)\n",
    "    img_jpgLIN = jpegLIN(img)\n",
    "    print(26*\"_\",\"\\nWith Threshold %.1f\" % threshold)\n",
    "    print(\"size with ZigZag: \" + total_size_jpeg(img_jpgZZ))\n",
    "    print(\"size w/o ZigZag:  \" + total_size_jpeg(img_jpgLIN))\n",
    "    print(\"-> ZigZag-Encoding makes image %.2f%% smaller\\n\" % (100*(1-(total_size_jpeg_nb(img_jpgZZ) / total_size_jpeg_nb(img_jpgLIN)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds = [0.5, 1., 2.]\n",
    "for threshold in thresholds:\n",
    "    compareZigZag(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZigZag-Encoding makes images smaller. But one can see, that with increasing threshold (hence smaller number of coefficents to encode) the ZigZag-Encoding can make less use of its advantage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare the image quality of the `Quantizise` vs. the `PickNthHighest` compressions. Make sure that the outputs \n",
    "# are roughly the same size. Why is one better then the other one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With roughly the same size in KB..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jpeg_pickNth = Jpeg([Blocks(8), DCTofBlocks(), PickNthHighest(n=10), ZigZack(8), Compress(np.int8)])\n",
    "jpeg_quant = Jpeg([Blocks(8), DCTofBlocks(), Quantizise(threshold=0.5), ZigZack(8), Compress(np.int8)])\n",
    "\n",
    "img_pickNth = jpeg_pickNth(img)\n",
    "print(\"Size with PickNthHighest:\", total_size_jpeg(img_pickNth))\n",
    "img_quant = jpeg_quant(img)\n",
    "print(\"Size with Quantization:\", total_size_jpeg(img_quant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_recon_pickNth = jpeg_pickNth.invert(img_pickNth) / 255.\n",
    "plt.imshow(img_recon_pickNth); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_recon_quant = jpeg_quant.invert(img_quant) / 255.\n",
    "plt.imshow(img_recon_quant); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(img_recon_pickNth[:200,100:300])\n",
    "plt.title(\"PickNthHighest\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(img_recon_quant[:200,100:300])\n",
    "plt.title(\"Quantization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite the same level of block-artifacts. But quantization-method yields better details. As one can see looking at her eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With roughly the same number of kept frequencies per block..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jpeg_pickNth = Jpeg([Blocks(8), DCTofBlocks(), PickNthHighest(n=5), ZigZack(8), Compress(np.int8)])\n",
    "jpeg_quant = Jpeg([Blocks(8), DCTofBlocks(), Quantizise(threshold=0.6), ZigZack(8), Compress(np.int8)])\n",
    "\n",
    "img_pickNth = jpeg_pickNth(img)\n",
    "print(\"Size with PickNthHighest:\", total_size_jpeg(img_pickNth))\n",
    "img_quant = jpeg_quant(img)\n",
    "print(\"Size with Quantizisation:\", total_size_jpeg(img_quant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(4.6 + 4.7 + 4.7) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(7.1 + 3.3 + 3.2) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_recon_pickNth = jpeg_pickNth.invert(img_pickNth) / 255.\n",
    "plt.imshow(img_recon_pickNth); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_recon_quant = jpeg_quant.invert(img_quant) / 255.\n",
    "plt.imshow(img_recon_quant); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow(img_recon_pickNth[:200,100:300])\n",
    "plt.title(\"PickNthHighest\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(img_recon_quant[:200,100:300])\n",
    "plt.title(\"Quantization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the quantization-method performs better here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantization-method has an overall better performance, because it leads to an irrelevancy reduction. Deciding which frequency to maintain and which not is optimized for human vision. Hence very high frequencies are maintained, which improves details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
